{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    'data_path': '../data/',\n",
    "    'model_path': '../model/',\n",
    "    'model_name': 'multi_task_learning',\n",
    "    'model_type': 'cnn1d',\n",
    "    \n",
    "    'valid_start_date_time': '2020-08-11 00',\n",
    "    'test_start_date_time': '2020-08-18 00',\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 100,\n",
    "    'es_patience': 10,\n",
    "    \n",
    "    'window_size': 7*24,\n",
    "    'shift': 1,\n",
    "    'target_length': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = pd.read_csv(CONFIGS['data_path']+'train.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (122400, 10)\n"
     ]
    }
   ],
   "source": [
    "data = deepcopy(train_origin)\n",
    "\n",
    "data.columns = [\n",
    "    'num', 'date_time', 'target', 'temp', 'wind',\n",
    "    'humid', 'rain', 'sun', 'non_elec_eq', 'sunlight_eq'\n",
    "]\n",
    "\n",
    "data['num'] -= 1\n",
    "\n",
    "print(f'data.shape: {data.shape}')\n",
    "\n",
    "CONFIGS['n_buildings'] = len(data['num'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42502fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_time_data(data):\n",
    "    \n",
    "    new_data = data.copy()\n",
    "\n",
    "    new_data['date_time'] = data['date_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H'))\n",
    "    \n",
    "    new_data['time_stamp'] = new_data['date_time'].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    new_data['year'] = new_data['date_time'].apply(lambda x: x.year)\n",
    "    new_data['month'] = new_data['date_time'].apply(lambda x: x.month)\n",
    "    new_data['day'] = new_data['date_time'].apply(lambda x: x.day)\n",
    "    \n",
    "    new_data['hour'] = new_data['date_time'].apply(lambda x: x.hour)\n",
    "    new_data['cos_hour'] = np.cos(2*np.pi*(new_data['hour']/24))\n",
    "    new_data['sin_hour'] = np.sin(2*np.pi*(new_data['hour']/24))\n",
    "\n",
    "    new_data['weekday'] = new_data['date_time'].apply(lambda x: x.weekday())\n",
    "    new_data['cos_weekday'] = np.cos(2*np.pi*(new_data['weekday']/7))\n",
    "    new_data['sin_weekday'] = np.sin(2*np.pi*(new_data['weekday']/7))\n",
    "    \n",
    "    new_data['is_holiday'] = 0\n",
    "    new_data.loc[(new_data['weekday'] == 5) | (new_data['weekday'] == 6), 'is_holiday'] = 1\n",
    "    new_data.loc[(new_data['month'] == 8) & (new_data['day'] == 17), 'is_holiday'] = 1\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4e23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = mk_time_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d6375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_building_info(data, data_for_cal):\n",
    "        \n",
    "    new_data = data.copy()\n",
    "    new_data['range'] = 0\n",
    "    new_data['mean'] = 0\n",
    "    new_data['std'] = 0\n",
    "    new_data['holiday_gap'] = 0\n",
    "    new_data['day_gap'] = 0\n",
    "    \n",
    "    B_NUM = 60\n",
    "\n",
    "    for num in range(B_NUM):\n",
    "        building = data_for_cal.query(f'num == {num}')\n",
    "        \n",
    "        bt_range = building['target'].max()-building['target'].min()\n",
    "        bt_mean = building['target'].mean()\n",
    "        bt_std = building['target'].std()\n",
    "        bt_holiday_gap = abs(building.query('is_holiday == 0')['target'].mean() - building.query('is_holiday == 1')['target'].mean())\n",
    "        bt_day_gap = 0\n",
    "        for d in range(building.shape[0]//24):\n",
    "            tmp = building['target'][d*24:(d+1)*24]\n",
    "            bt_day_gap += (tmp.max()-tmp.min())/(building.shape[0]//24)\n",
    "            \n",
    "        new_data.loc[new_data['num']==num, 'range'] = bt_range\n",
    "        new_data.loc[new_data['num']==num, 'mean'] = bt_mean\n",
    "        new_data.loc[new_data['num']==num, 'std'] = bt_std\n",
    "        new_data.loc[new_data['num']==num, 'holiday_gap'] = bt_holiday_gap\n",
    "        new_data.loc[new_data['num']==num, 'day_gap'] = bt_day_gap\n",
    "        \n",
    "    new_data['mean_to_inverse'] = new_data['mean']\n",
    "    new_data['std_to_inverse'] = new_data['std']\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebaea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = mk_building_info(new_data, new_data[new_data['date_time']<CONFIGS['valid_start_date_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8655ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_mean_std_dict(data, scaling_by_building_cols):\n",
    "    mean_std_dict = {}\n",
    "    for num in range(60):\n",
    "        building = data.query(f'num == {num}')\n",
    "        mean_std_dict[num] = {\n",
    "            col: {\n",
    "                'mean': building[col].mean(),\n",
    "                'std': building[col].std()\n",
    "            } for col in scaling_by_building_cols\n",
    "        }\n",
    "    return mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d91d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a2d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01506353",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_by_building_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp', 'target',\n",
    "]\n",
    "scaling_by_all_cols = ['range', 'mean', 'std', 'holiday_gap', 'day_gap']\n",
    "\n",
    "mean_std_dict = mk_mean_std_dict(\n",
    "    new_data[new_data['date_time'] < CONFIGS['valid_start_date_time']],\n",
    "    scaling_by_building_cols\n",
    ")\n",
    "CONFIGS['mean_std_dict'] = mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d134a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaling(data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict=None):\n",
    "    if not mean_std_dict:\n",
    "        mean_std_dict = mk_mean_std_dict(data, scaling_by_building_cols)\n",
    "        \n",
    "    new_data = data.copy()\n",
    "    for num in range(60):\n",
    "        for col in scaling_by_building_cols:\n",
    "            new_data.loc[new_data['num']==num, col] -= mean_std_dict[num][col]['mean']\n",
    "            new_data.loc[new_data['num']==num, col] /= mean_std_dict[num][col]['std']\n",
    "    \n",
    "    for col in scaling_by_all_cols:\n",
    "        m = new_data.loc[:, col].mean()\n",
    "        s = new_data.loc[:, col].std()\n",
    "        new_data.loc[:, col] -= m\n",
    "        new_data.loc[:, col] /= s\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6674807",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = standard_scaling(new_data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b337cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_num_cols = ['num']\n",
    "building_info_cols = ['range', 'mean', 'std', 'holiday_gap', 'day_gap']\n",
    "target_time_info_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',\n",
    "    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',\n",
    "    'is_holiday',\n",
    "]\n",
    "time_series_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',\n",
    "    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',\n",
    "    'is_holiday', 'target',\n",
    "]\n",
    "target_cols = ['target']\n",
    "to_inverse_cols = ['mean_to_inverse', 'std_to_inverse']\n",
    "input_cols = list(set(\n",
    "    building_num_cols + building_info_cols + target_time_info_cols +\n",
    "    time_series_cols + target_cols + to_inverse_cols\n",
    "))\n",
    "\n",
    "\n",
    "CONFIGS['building_num_cols'] = building_num_cols\n",
    "CONFIGS['building_info_cols'] = building_info_cols\n",
    "CONFIGS['target_time_info_cols'] = target_time_info_cols\n",
    "CONFIGS['time_series_cols'] = time_series_cols\n",
    "CONFIGS['target_cols'] = target_cols\n",
    "CONFIGS['to_inverse_cols'] = to_inverse_cols\n",
    "CONFIGS['input_cols'] = input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda65897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(data, CONFIGS):\n",
    "    croped = data[CONFIGS['window_size']+1:-(CONFIGS['target_length']-2)]\n",
    "    return Dataset.from_tensor_slices(croped)\n",
    "\n",
    "\n",
    "def mk_window(data, size, shift):\n",
    "    ds = Dataset.from_tensor_slices(data)\n",
    "    ds = ds.window(\n",
    "        size, shift=shift, drop_remainder=True\n",
    "    ).flat_map(lambda x: x).batch(size)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def mk_dataset(data, CONFIGS, shuffle=False):\n",
    "\n",
    "    data = data[CONFIGS['input_cols']]\n",
    "    building_length = data.query('num == 0').shape[0]\n",
    "\n",
    "    building_num = data[CONFIGS['building_num_cols']]\n",
    "    building_info = data[CONFIGS['building_info_cols']]\n",
    "    target_time_info = data[CONFIGS['target_time_info_cols']]\n",
    "    time_series = data[CONFIGS['time_series_cols']]\n",
    "    to_inverse = data[CONFIGS['to_inverse_cols']]\n",
    "    target = data[CONFIGS['target_cols']]\n",
    "\n",
    "    # building_num\n",
    "    building_num_ds = Dataset.from_tensor_slices(building_num).batch(building_length)\n",
    "    building_num_ds = building_num_ds.flat_map(lambda x: crop(x, CONFIGS))\n",
    "    building_num_ds = building_num_ds.map(lambda x: tf.cast(x, tf.int16))\n",
    "\n",
    "    # building_info\n",
    "    building_info_ds = Dataset.from_tensor_slices(building_info).batch(building_length)\n",
    "    building_info_ds = building_info_ds.flat_map(lambda x: crop(x, CONFIGS))\n",
    "    building_info_ds = building_info_ds.map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # target_time_info\n",
    "    target_time_info_ds = Dataset.from_tensor_slices(target_time_info).batch(building_length)\n",
    "    target_time_info_ds = target_time_info_ds.flat_map(lambda x: crop(x, CONFIGS))\n",
    "    target_time_info_ds = target_time_info_ds.map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # time_series\n",
    "    time_series_ds = Dataset.from_tensor_slices(time_series).batch(building_length)\n",
    "    time_series_ds = time_series_ds.flat_map(\n",
    "        lambda x: mk_window(x, CONFIGS['window_size'], CONFIGS['shift']))\n",
    "    time_series_ds = time_series_ds.map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # target\n",
    "    target_ds = Dataset.from_tensor_slices(target).batch(building_length)\n",
    "    target_ds = target_ds.flat_map(\n",
    "        lambda x: mk_window(x, CONFIGS['target_length'], CONFIGS['shift']))\n",
    "    target_ds = target_ds.map(lambda x: tf.cast(x, tf.float32))\n",
    "    \n",
    "    # to_inverse\n",
    "    to_inverse_ds = Dataset.from_tensor_slices(to_inverse).batch(building_length)\n",
    "    to_inverse_ds = to_inverse_ds.flat_map(lambda x: crop(x, CONFIGS))\n",
    "    to_inverse_ds = to_inverse_ds.map(lambda x: tf.cast(x, tf.int16))\n",
    "    \n",
    "    # zip\n",
    "    ds = Dataset.zip((\n",
    "        (\n",
    "            building_num_ds,\n",
    "            building_info_ds,\n",
    "            target_time_info_ds,\n",
    "            time_series_ds,\n",
    "            to_inverse_ds\n",
    "        ),\n",
    "        target_ds\n",
    "    ))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(512)\n",
    "    ds = ds.batch(CONFIGS['batch_size']).cache().prefetch(2)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0107708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_dt = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H')\n",
    "hour_to_td = lambda x: datetime.timedelta(hours=x)\n",
    "\n",
    "train = new_data.loc[\n",
    "    new_data['date_time'] < \\\n",
    "        str_to_dt(CONFIGS['valid_start_date_time']),\n",
    "    :\n",
    "]\n",
    "valid = new_data.loc[\n",
    "    (new_data['date_time'] > \\\n",
    "        str_to_dt(CONFIGS['valid_start_date_time'])-hour_to_td(CONFIGS['window_size']))&\\\n",
    "    (new_data['date_time'] < \\\n",
    "         str_to_dt(CONFIGS['test_start_date_time'])),\n",
    "    :\n",
    "]\n",
    "test = new_data.loc[\n",
    "    new_data['date_time'] > \\\n",
    "        str_to_dt(CONFIGS['test_start_date_time'])-hour_to_td(CONFIGS['window_size']),\n",
    "    :\n",
    "]\n",
    "\n",
    "train_ds = mk_dataset(train, CONFIGS, shuffle=True)\n",
    "valid_ds = mk_dataset(valid, CONFIGS)\n",
    "test_ds = mk_dataset(test, CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11cfa087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(1,), dtype=int16, numpy=array([0], dtype=int16)>,\n",
       "  <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "  array([-0.78192174,  3.4315276 , -0.7918083 , -0.6576589 , -0.9102245 ],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(11,), dtype=float32, numpy=\n",
       "  array([ 0.37290826, -1.5672512 ,  0.8200607 , -0.2154803 , -0.5449863 ,\n",
       "          2.0780547 ,  0.8660254 ,  0.5       ,  0.6234898 ,  0.7818315 ,\n",
       "          0.        ], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(168, 12), dtype=float32, numpy=\n",
       "  array([[ 0.09663188, -1.2139744 ,  1.3741381 , ...,  0.7818315 ,\n",
       "           0.        ,  0.7294189 ],\n",
       "         [ 0.12732926, -1.1256552 ,  1.4357023 , ...,  0.7818315 ,\n",
       "           0.        ,  0.513595  ],\n",
       "         [ 0.0659345 ,  0.9940057 ,  1.4357023 , ...,  0.7818315 ,\n",
       "           0.        ,  0.60095227],\n",
       "         ...,\n",
       "         [ 0.89476365, -1.037336  ,  0.38911152, ...,  0.        ,\n",
       "           1.        ,  1.2741172 ],\n",
       "         [ 0.77197415, -1.2139744 ,  0.5122399 , ...,  0.        ,\n",
       "           1.        ,  1.3306426 ],\n",
       "         [ 0.61848724, -1.3022935 ,  0.573804  , ...,  0.7818315 ,\n",
       "           0.        ,  1.3511972 ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(2,), dtype=int16, numpy=array([8520,  126], dtype=int16)>),\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[0.7294189 ],\n",
       "        [0.513595  ],\n",
       "        [0.60095227]], dtype=float32)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(test_ds.unbatch()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db5766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87e40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23086a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9046abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS['target_max'] = \\\n",
    "    data[data['date_time']<CONFIGS['valid_start_date_time']]['target'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e30e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRMSE(Loss):\n",
    "    def __init__(self, CONFIGS, name=\"custom_rmse\"):\n",
    "        super(CustomRMSE, self).__init__(name=name)\n",
    "        self.CONFIGS = CONFIGS\n",
    "        self.target_mean_std = tf.cast(pd.DataFrame(\n",
    "            [CONFIGS['mean_std_dict'][i]['target'] for i in range(60)]\n",
    "        ).values, tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        def tmp(num):\n",
    "            print(num)\n",
    "            return self.target_mean_std[0]\n",
    "        means_stds = tf.map_fn(\n",
    "            tmp,\n",
    "            y_pred[1]\n",
    "        )\n",
    "\n",
    "        y_true_inversed_scaled = y_pred[0] * tf.reshape(means_stds[:, 1], (-1, 1))\n",
    "        y_true_inversed_scaled = y_true_inversed_scaled + tf.reshape(means_stds[:, 0], (-1, 1))\n",
    "        y_true_inversed_scaled = y_true_inversed_scaled/self.CONFIGS['target_max']\n",
    "        y_pred_inversed_scaled = y_true * tf.reshape(means_stds[:, 1], (-1, 1))\n",
    "        y_pred_inversed_scaled = y_pred_inversed_scaled + tf.reshape(means_stds[:, 0], (-1, 1))\n",
    "        y_pred_inversed_scaled = y_pred_inversed_scaled/self.CONFIGS['target_max']\n",
    "\n",
    "        rmse = (tf.reduce_mean((y_true_inversed_scaled - y_pred_inversed_scaled)**2))**0.5\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9661c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c05db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0db0bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(CONFIGS, model_name=None, print_summary=False):\n",
    "    \n",
    "    # building_num\n",
    "    building_num_inputs = Input(batch_shape=(None, 1), name='building_num_inputs')\n",
    "    input_dim = CONFIGS['n_buildings']; output_dim = CONFIGS['embedding_dim']\n",
    "    building_num_emb = Embedding(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        name='embedding'\n",
    "    )(building_num_inputs)\n",
    "    bn_outputs = Reshape(target_shape=(output_dim,), name='bn_outputs')(building_num_emb)\n",
    "    \n",
    "    # building_info\n",
    "    building_info_inputs = Input(\n",
    "        batch_shape=(None, len(CONFIGS['building_info_cols'])),\n",
    "        name='building_info_inputs'\n",
    "    )\n",
    "    bi_dense_0 = Dense(16, activation='relu', name='bi_dense_0')(building_info_inputs)\n",
    "    bi_outputs = Dense(32, activation='relu', name='bi_outputs')(bi_dense_0)\n",
    "    \n",
    "    # target_time_info\n",
    "    target_time_info_inputs = Input(\n",
    "        batch_shape=(None, len(CONFIGS['target_time_info_cols'])),\n",
    "        name='target_time_info_inputs'\n",
    "    )\n",
    "    tti_dense_0 = Dense(16, activation='relu', name='tti_dense_0')(target_time_info_inputs)\n",
    "    tti_outputs = Dense(32, activation='relu', name='tti_outputs')(tti_dense_0)\n",
    "    \n",
    "    # time_series\n",
    "    time_series_inputs = Input(batch_shape=(\n",
    "        None, CONFIGS['window_size'], len(CONFIGS['time_series_cols'])\n",
    "    ), name='time_series_inputs')\n",
    "    \n",
    "    if CONFIGS['model_type'] == 'flatten':\n",
    "        time_series_outputs = Flatten(name='time_series_outputs')(time_series_inputs)\n",
    "    elif CONFIGS['model_type'] == 'cnn1d':\n",
    "        conv_0 = Conv1D(16, 3, 2, activation='relu', name='conv_0')(time_series_inputs)\n",
    "        pool_0 = MaxPool1D(2, name='pool_0')(conv_0)\n",
    "        conv_1 = Conv1D(32, 3, 2, activation='relu', name='conv_1')(pool_0)\n",
    "        pool_1 = MaxPool1D(2, name='pool_1')(conv_1)\n",
    "        time_series_outputs = Flatten(name='time_series_outputs')(pool_1)\n",
    "    elif CONFIGS['model_type'] == 'cnn2d':\n",
    "        reshape = Reshape(target_shape=(\n",
    "            CONFIGS['window_size'], len(CONFIGS['time_series_cols']), 1\n",
    "        ), name='reshape')(time_series_inputs)\n",
    "        conv_0 = Conv2D(8, (3, 1), strides=(2, 1), activation='relu', name='conv_0')(reshape)\n",
    "        pool_0 = MaxPool2D((2, 1), name='pool_0')(conv_0)\n",
    "        conv_1 = Conv2D(16, (3, 1), strides=(2, 1), activation='relu', name='conv_1')(pool_0)\n",
    "        pool_1 = MaxPool2D((2, 1), name='pool_1')(conv_1)\n",
    "        time_series_outputs = Flatten(name='time_series_outputs')(pool_1)\n",
    "    elif CONFIGS['model_type'] == 'lstm':\n",
    "        lstm_0 = LSTM(16, return_sequences=True, activation='relu', name='lstm_0')(time_series_inputs)\n",
    "        lstm_1 = LSTM(32, activation='relu', name='lstm_1')(lstm_0)\n",
    "        time_series_outputs = Flatten(name='time_series_outputs')(lstm_1)\n",
    "    elif CONFIGS['model_type'] == 'bilstm':\n",
    "        bilstm_0 = Bidirectional(LSTM(\n",
    "            16, return_sequences=True, activation='relu'\n",
    "        ), name='bilstm_0')(time_series_inputs)\n",
    "        bilstm_1 = Bidirectional(LSTM(\n",
    "            32, activation='relu'\n",
    "        ), name='bilstm_1')(bilstm_0)\n",
    "        time_series_outputs = Flatten(name='time_series_outputs')(bilstm_1)\n",
    "    \n",
    "    concat = Concatenate(name='concat')([bn_outputs, bi_outputs, tti_outputs, time_series_outputs])\n",
    "        \n",
    "    dense_0 = Dense(64, activation='relu', name='dense_0')(concat)\n",
    "    dense_1 = Dense(32, activation='relu', name='dense_1')(dense_0)\n",
    "    outputs = Dense(CONFIGS['target_length'], name='outputs')(dense_1)\n",
    "    \n",
    "    if not model_name:\n",
    "        model_name = CONFIGS['model_name']\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = [\n",
    "            building_num_inputs,\n",
    "            building_info_inputs,\n",
    "            target_time_info_inputs,\n",
    "            time_series_inputs\n",
    "        ],\n",
    "        outputs = [outputs, building_num_inputs],\n",
    "        name = model_name\n",
    "    )\n",
    "    \n",
    "    optimizer = Adam(learning_rate=CONFIGS['learning_rate'])\n",
    "    model.compile(\n",
    "        loss = CustomRMSE(CONFIGS),\n",
    "        optimizer = optimizer,\n",
    "    )\n",
    "    \n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ace97d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"multi_task_learning\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "time_series_inputs (InputLayer) [(None, 168, 12)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv1D)                 (None, 83, 16)       592         time_series_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pool_0 (MaxPooling1D)           (None, 41, 16)       0           conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "building_num_inputs (InputLayer [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "building_info_inputs (InputLaye [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_time_info_inputs (InputL [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 20, 32)       1568        pool_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 30)        1800        building_num_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bi_dense_0 (Dense)              (None, 16)           96          building_info_inputs[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tti_dense_0 (Dense)             (None, 16)           192         target_time_info_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (MaxPooling1D)           (None, 10, 32)       0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_outputs (Reshape)            (None, 30)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_outputs (Dense)              (None, 32)           544         bi_dense_0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tti_outputs (Dense)             (None, 32)           544         tti_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_series_outputs (Flatten)   (None, 320)          0           pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 414)          0           bn_outputs[0][0]                 \n",
      "                                                                 bi_outputs[0][0]                 \n",
      "                                                                 tti_outputs[0][0]                \n",
      "                                                                 time_series_outputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 64)           26560       concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 3)            99          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 34,075\n",
      "Trainable params: 34,075\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CONFIGS['embedding_dim'] = 30\n",
    "\n",
    "model = set_model(CONFIGS, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a889564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, valid_ds, CONFIGS):\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        patience=CONFIGS['es_patience']\n",
    "    )\n",
    "    save_best_only = ModelCheckpoint(\n",
    "        filepath = f'{CONFIGS[\"model_path\"]}{CONFIGS[\"model_name\"]}.h5',\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        batch_size = CONFIGS['batch_size'],\n",
    "        epochs = CONFIGS['epochs'],\n",
    "        validation_data = valid_ds,\n",
    "        callbacks = [\n",
    "            early_stop,\n",
    "            save_best_only,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdb115f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"custom_rmse/map/while/TensorArrayV2Read/TensorListGetItem:0\", shape=(), dtype=float32)\n",
      "Tensor(\"custom_rmse_1/map/while/TensorArrayV2Read/TensorListGetItem:0\", shape=(), dtype=float32)\n",
      "Tensor(\"custom_rmse/map/while/TensorArrayV2Read/TensorListGetItem:0\", shape=(), dtype=float32)\n",
      "Tensor(\"custom_rmse_1/map/while/TensorArrayV2Read/TensorListGetItem:0\", shape=(), dtype=float32)\n",
      "    202/Unknown - 3s 8ms/step - loss: 0.0371 - outputs_loss: 0.0076 - building_num_inputs_loss: 0.0295"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e6b2d524b3be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-49153d0b7b1c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_ds, valid_ds, CONFIGS)\u001b[0m\n\u001b[1;32m     18\u001b[0m         callbacks = [\n\u001b[1;32m     19\u001b[0m             \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0msave_best_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         ]\n\u001b[1;32m     22\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m     \u001b[0mcache_key_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, cache_key_context, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   3181\u001b[0m       input_signature = pywrap_tfe.TFE_Py_EncodeArg(inputs,\n\u001b[1;32m   3182\u001b[0m                                                     include_tensor_ranks_only)\n\u001b[0;32m-> 3183\u001b[0;31m       \u001b[0mhashable_input_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_input_signature_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3185\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_make_input_signature_hashable\u001b[0;34m(elem)\u001b[0m\n\u001b[1;32m    112\u001b[0m   \"\"\"\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# TODO(slebedev): consider using nest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model(model, train_ds, valid_ds, CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbdf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fcbd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = set_model(CONFIGS, model_name='best_'+CONFIGS['model_name'])\n",
    "best_model.load_weights(f'{CONFIGS[\"model_path\"]}{CONFIGS[\"model_name\"]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679da351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.264660\ttrain_rmse: 1078.519767\n",
      "valid_loss: 2.009985\tvalid_rmse: 1240.208086\n",
      "test_loss: 2.189661\ttest_rmse: 1375.793010\n"
     ]
    }
   ],
   "source": [
    "train_loss = best_model.evaluate(train_ds, verbose=0)\n",
    "valid_loss = best_model.evaluate(valid_ds, verbose=0)\n",
    "test_loss = best_model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "train_rmse = inversed_rmse(train_ds, best_model)\n",
    "valid_rmse = inversed_rmse(valid_ds, best_model)\n",
    "test_rmse = inversed_rmse(test_ds, best_model)\n",
    "\n",
    "print(f'train_loss: {train_loss:.6f}\\ttrain_rmse: {train_rmse:.6f}')\n",
    "print(f'valid_loss: {valid_loss:.6f}\\tvalid_rmse: {valid_rmse:.6f}')\n",
    "print(f'test_loss: {test_loss:.6f}\\ttest_rmse: {test_rmse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70614361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
