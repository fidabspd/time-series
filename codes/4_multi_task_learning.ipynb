{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    'data_path': '../data/',\n",
    "    'model_path': '../model/',\n",
    "    'model_name': 'multi_task_learning',\n",
    "    'model_type': 'cnn1d',\n",
    "    \n",
    "    'valid_start_date_time': '2020-08-11 00',\n",
    "    'test_start_date_time': '2020-08-18 00',\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 100,\n",
    "    'es_patience': 10,\n",
    "    \n",
    "    'window_size': 7*24,\n",
    "    'target_length': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = pd.read_csv(CONFIGS['data_path']+'train.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (122400, 10)\n"
     ]
    }
   ],
   "source": [
    "data = deepcopy(train_origin)\n",
    "\n",
    "data.columns = [\n",
    "    'num', 'date_time', 'target', 'temp', 'wind',\n",
    "    'humid', 'rain', 'sun', 'non_elec_eq', 'sunlight_eq'\n",
    "]\n",
    "\n",
    "data['num'] -= 1\n",
    "\n",
    "print(f'data.shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42502fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_time_data(data):\n",
    "    \n",
    "    new_data = data.copy()\n",
    "\n",
    "    new_data['date_time'] = data['date_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H'))\n",
    "    \n",
    "    new_data['time_stamp'] = new_data['date_time'].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    new_data['year'] = new_data['date_time'].apply(lambda x: x.year)\n",
    "    new_data['month'] = new_data['date_time'].apply(lambda x: x.month)\n",
    "    new_data['day'] = new_data['date_time'].apply(lambda x: x.day)\n",
    "    \n",
    "    new_data['hour'] = new_data['date_time'].apply(lambda x: x.hour)\n",
    "    new_data['cos_hour'] = np.cos(2*np.pi*(new_data['hour']/24))\n",
    "    new_data['sin_hour'] = np.sin(2*np.pi*(new_data['hour']/24))\n",
    "\n",
    "    new_data['weekday'] = new_data['date_time'].apply(lambda x: x.weekday())\n",
    "    new_data['cos_weekday'] = np.cos(2*np.pi*(new_data['weekday']/7))\n",
    "    new_data['sin_weekday'] = np.sin(2*np.pi*(new_data['weekday']/7))\n",
    "    \n",
    "    new_data['is_holiday'] = 0\n",
    "    new_data.loc[(new_data['weekday'] == 5) | (new_data['weekday'] == 6), 'is_holiday'] = 1\n",
    "    new_data.loc[(new_data['month'] == 8) & (new_data['day'] == 17), 'is_holiday'] = 1\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4e23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = mk_time_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebaea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_building_info(data, data_for_cal):\n",
    "        \n",
    "    new_data = data.copy()\n",
    "    new_data['range'] = 0\n",
    "    new_data['mean'] = 0\n",
    "    new_data['std'] = 0\n",
    "    new_data['holiday_gap'] = 0\n",
    "    new_data['day_gap'] = 0\n",
    "    \n",
    "    B_NUM = 60\n",
    "\n",
    "    for num in range(B_NUM):\n",
    "        building = data_for_cal.query(f'num == {num}')\n",
    "        \n",
    "        bt_range = building['target'].max()-building['target'].min()\n",
    "        bt_mean = building['target'].mean()\n",
    "        bt_std = building['target'].std()\n",
    "        bt_holiday_gap = abs(building.query('is_holiday == 0')['target'].mean() - building.query('is_holiday == 1')['target'].mean())\n",
    "        bt_day_gap = 0\n",
    "        for d in range(building.shape[0]//24):\n",
    "            tmp = building['target'][d*24:(d+1)*24]\n",
    "            bt_day_gap += (tmp.max()-tmp.min())/(building.shape[0]//24)\n",
    "            \n",
    "        new_data.loc[new_data['num']==num, 'range'] = bt_range\n",
    "        new_data.loc[new_data['num']==num, 'mean'] = bt_mean\n",
    "        new_data.loc[new_data['num']==num, 'std'] = bt_std\n",
    "        new_data.loc[new_data['num']==num, 'holiday_gap'] = bt_holiday_gap\n",
    "        new_data.loc[new_data['num']==num, 'day_gap'] = bt_day_gap\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "new_data = mk_building_info(new_data, new_data[new_data['date_time']<CONFIGS['valid_start_date_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8655ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_mean_std_dict(data, scaling_by_building_cols):\n",
    "    mean_std_dict = {}\n",
    "    for num in range(60):\n",
    "        building = data.query(f'num == {num}')\n",
    "        mean_std_dict[num] = {\n",
    "            col: {\n",
    "                'mean': building[col].mean(),\n",
    "                'std': building[col].std()\n",
    "            } for col in scaling_by_building_cols\n",
    "        }\n",
    "    return mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01506353",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_by_building_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp', 'target',\n",
    "]\n",
    "scaling_by_all_cols = ['range', 'mean', 'std', 'holiday_gap', 'day_gap']\n",
    "\n",
    "mean_std_dict = mk_mean_std_dict(\n",
    "    new_data[new_data['date_time'] < CONFIGS['valid_start_date_time']],\n",
    "    scaling_by_building_cols\n",
    ")\n",
    "CONFIGS['mean_std_dict'] = mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d134a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaling(data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict=None):\n",
    "    if not mean_std_dict:\n",
    "        mean_std_dict = mk_mean_std_dict(data, scaling_by_building_cols)\n",
    "        \n",
    "    new_data = data.copy()\n",
    "    for num in range(60):\n",
    "        for col in scaling_by_building_cols:\n",
    "            new_data.loc[new_data['num']==num, col] -= mean_std_dict[num][col]['mean']\n",
    "            new_data.loc[new_data['num']==num, col] /= mean_std_dict[num][col]['std']\n",
    "    \n",
    "    for col in scaling_by_all_cols:\n",
    "        m = new_data.loc[:, col].mean()\n",
    "        s = new_data.loc[:, col].std()\n",
    "        new_data.loc[:, col] -= m\n",
    "        new_data.loc[:, col] /= s\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6674807",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = standard_scaling(new_data, scaling_by_building_cols, scaling_by_all_cols, mean_std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b337cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_num_cols = ['num']\n",
    "building_info_cols = ['range', 'mean', 'std', 'holiday_gap', 'day_gap']\n",
    "target_time_info_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',\n",
    "    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',\n",
    "    'is_holiday',\n",
    "]\n",
    "time_series_cols = [\n",
    "    'temp', 'wind', 'humid', 'rain', 'sun', 'time_stamp',\n",
    "    'cos_hour', 'sin_hour', 'cos_weekday', 'sin_weekday',\n",
    "    'is_holiday', 'target',\n",
    "]\n",
    "target_cols = ['target']\n",
    "input_cols = list(set(\n",
    "    building_num_cols+building_info_cols+\n",
    "    target_time_info_cols+time_series_cols+target_cols\n",
    "))\n",
    "\n",
    "\n",
    "CONFIGS['building_num_cols'] = building_num_cols\n",
    "CONFIGS['building_info_cols'] = building_info_cols\n",
    "CONFIGS['target_time_info_cols'] = target_time_info_cols\n",
    "CONFIGS['time_series_cols'] = time_series_cols\n",
    "CONFIGS['target_cols'] = target_cols\n",
    "CONFIGS['input_cols'] = input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "466af073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dataset(data, CONFIGS, shuffle=False):\n",
    "\n",
    "    data = data[CONFIGS['input_cols']]\n",
    "    building_length = data.query('num == 0').shape[0]\n",
    "\n",
    "    building_num = data[CONFIGS['building_num_cols']]\n",
    "    building_info = data[CONFIGS['building_info_cols']]\n",
    "    target_time_info = data[CONFIGS['target_time_info_cols']]\n",
    "    time_series = data[CONFIGS['time_series_cols']]\n",
    "    target = data[CONFIGS['target_cols']]\n",
    "\n",
    "    # building_num\n",
    "    building_num_ds = Dataset.from_tensor_slices(building_num).batch(building_length)\n",
    "    building_num_ds = building_num_ds.flat_map(\n",
    "        lambda x: Dataset.from_tensor_slices(\n",
    "            x[CONFIGS['window_size']+1:-(CONFIGS['target_length']-2)]\n",
    "        )\n",
    "    ).map(lambda x: tf.cast(x, tf.int16))\n",
    "\n",
    "    # building_info\n",
    "    building_info_ds = Dataset.from_tensor_slices(building_info).batch(building_length)\n",
    "    building_info_ds = building_info_ds.flat_map(\n",
    "        lambda x: Dataset.from_tensor_slices(\n",
    "            x[CONFIGS['window_size']+1:-(CONFIGS['target_length']-2)]\n",
    "        )\n",
    "    ).map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # target_time_info\n",
    "    target_time_info_ds = Dataset.from_tensor_slices(target_time_info).batch(building_length)\n",
    "    target_time_info_ds = target_time_info_ds.flat_map(\n",
    "        lambda x: Dataset.from_tensor_slices(\n",
    "            x[CONFIGS['window_size']+1:-(CONFIGS['target_length']-2)]\n",
    "        )\n",
    "    ).map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # time_series\n",
    "    time_series_ds = Dataset.from_tensor_slices(time_series).batch(building_length)\n",
    "    time_series_ds = time_series_ds.flat_map(\n",
    "        lambda x:\n",
    "            Dataset.from_tensor_slices(x).window(\n",
    "                CONFIGS['window_size'], shift=1, drop_remainder=True\n",
    "            ).flat_map(lambda x: x).batch(CONFIGS['window_size'])\n",
    "    ).map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # target\n",
    "    target_ds = Dataset.from_tensor_slices(target).batch(building_length)\n",
    "    target_ds = target_ds.flat_map(\n",
    "        lambda x:\n",
    "            Dataset.from_tensor_slices(x).window(\n",
    "                CONFIGS['target_length'], shift=1, drop_remainder=True\n",
    "            ).flat_map(lambda x: x).batch(CONFIGS['target_length'])\n",
    "    ).map(lambda x: tf.cast(x, tf.float32))\n",
    "\n",
    "    # zip\n",
    "    ds = Dataset.zip((\n",
    "        (\n",
    "            building_num_ds,\n",
    "            building_info_ds,\n",
    "            target_time_info_ds,\n",
    "            time_series_ds\n",
    "        ),\n",
    "        target_ds\n",
    "    ))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(512)\n",
    "    ds = ds.batch(CONFIGS['batch_size']).cache().prefetch(2)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0107708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_dt = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H')\n",
    "hour_to_td = lambda x: datetime.timedelta(hours=x)\n",
    "\n",
    "train = new_data.loc[\n",
    "    new_data['date_time'] < \\\n",
    "        str_to_dt(CONFIGS['valid_start_date_time']),\n",
    "    :\n",
    "]\n",
    "valid = new_data.loc[\n",
    "    (new_data['date_time'] > \\\n",
    "        str_to_dt(CONFIGS['valid_start_date_time'])-hour_to_td(CONFIGS['window_size']))&\\\n",
    "    (new_data['date_time'] < \\\n",
    "         str_to_dt(CONFIGS['test_start_date_time'])),\n",
    "    :\n",
    "]\n",
    "test = new_data.loc[\n",
    "    new_data['date_time'] > \\\n",
    "        str_to_dt(CONFIGS['test_start_date_time'])-hour_to_td(CONFIGS['window_size']),\n",
    "    :\n",
    "]\n",
    "\n",
    "train_ds = mk_dataset(train, CONFIGS, shuffle=True)\n",
    "valid_ds = mk_dataset(valid, CONFIGS)\n",
    "test_ds = mk_dataset(test, CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversed_rmse 만들면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04206e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
